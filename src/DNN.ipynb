{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import config\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "from tensorflow.contrib.layers import batch_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_features(data_format='train'):\n",
    "    feature_df = None\n",
    "    id_field = 'id' if data_format == 'train' else 'test_id'\n",
    "    for root, dirs, filenames in os.walk(os.path.join(config.FEATURE_DIR, data_format)):\n",
    "        for filename in filenames:\n",
    "            if filename.endswith('.csv'):\n",
    "                df = pd.read_csv(os.path.join(root, filename))\n",
    "                if feature_df is None:\n",
    "                    feature_df = df\n",
    "                else:\n",
    "                    feature_df = feature_df.merge(df, on=id_field)\n",
    "    return feature_df\n",
    "\n",
    "def load_target():\n",
    "    target_df = pd.read_csv(os.path.join(config.DATA_DIR, 'train.csv'))[['id', 'is_duplicate']]\n",
    "    return target_df\n",
    "\n",
    "def log_loss_dup(y_true, y_pred):\n",
    "    return log_loss(y_true, y_pred[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_features = load_features().drop('id', 1).as_matrix()\n",
    "train_target = load_target().drop('id', 1).as_matrix()[:, 0]\n",
    "dev_train_features, dev_test_features, dev_train_target, dev_test_target = train_test_split(\n",
    "    train_features, train_target, test_size=config.TEST_SIZE, random_state=config.RANDOM_SEED\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(tf.constant(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NeuralNet:\n",
    "    # 2 layers hidden unit\n",
    "    def __init__(self, h=100, epoch=1000, learning_rate=0.5, batch_size=100, activate_f='relu'):\n",
    "        self.h = h\n",
    "        self.epoch = epoch\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.activate_f = activate_f\n",
    "        self.tf_X = None\n",
    "        self.is_training = None\n",
    "        self.W1 = None\n",
    "        self.W2 = None\n",
    "        self.W3 = None\n",
    "        self.b1 = None\n",
    "        self.b2 = None\n",
    "        self.b3 = None\n",
    "        self.out = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        y = self._binarize_labels(y)\n",
    "        size = X.shape[0]\n",
    "        np.random.seed(config.RANDOM_SEED)\n",
    "        index = np.random.permutation(range(size))\n",
    "        d = X.shape[1]\n",
    "        c = y.shape[1]\n",
    "\n",
    "        tf_X = tf.placeholder(tf.float32, [None, d])\n",
    "        tf_y = tf.placeholder(tf.float32, [None, c])\n",
    "        is_training = tf.placeholder(tf.bool)\n",
    "        W1 = tf.Variable(tf.random_normal([d, self.h], seed=config.RANDOM_SEED)/d)\n",
    "        b1 = tf.Variable(tf.zeros(self.h))\n",
    "        W2 = tf.Variable(tf.random_normal([self.h, self.h], seed=config.RANDOM_SEED)/self.h)\n",
    "        b2 = tf.Variable(tf.zeros(self.h))\n",
    "        W3 = tf.Variable(tf.random_normal([self.h, c], seed=config.RANDOM_SEED)/self.h)\n",
    "        b3 = tf.Variable(tf.zeros(c))\n",
    "\n",
    "        z1 = tf.matmul(tf_X, W1) + b1\n",
    "        z1 = batch_norm(z1, center=True, scale=True, is_training=is_training, updates_collections=None)\n",
    "        a1 = self.activation_func(z1)\n",
    "        z2 = tf.matmul(a1, W2) + b2\n",
    "        z2 = batch_norm(z2, center=True, scale=True, is_training=is_training, updates_collections=None)\n",
    "        a2 = self.activation_func(z2)\n",
    "        z3 = tf.matmul(a2, W3) + b3\n",
    "\n",
    "        cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=z3, labels=tf_y))\n",
    "\n",
    "        train_step = tf.train.GradientDescentOptimizer(self.learning_rate).minimize(cross_entropy)\n",
    "\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        start_i = 0\n",
    "        for i in range(self.epoch):\n",
    "            start_i, batch_index = self._next_batch(start_i, self.batch_size, index)\n",
    "            start_time = time.time()\n",
    "            sess.run(train_step, feed_dict={tf_X: X[batch_index], tf_y: y[batch_index], is_training: True})\n",
    "            if ((i+1) % (self.epoch / 10) == 0):\n",
    "                print \"Epoch %d: %fs. Loss: %f\" % (i+1, time.time()-start_time, sess.run(cross_entropy, feed_dict={\n",
    "                    tf_X: X[batch_index], tf_y: y[batch_index], is_training: False\n",
    "                }))\n",
    "\n",
    "        self.W1 = W1\n",
    "        self.b1 = b1\n",
    "        self.W2 = W2\n",
    "        self.b2 = b2\n",
    "        self.W3 = W3\n",
    "        self.b3 = b3\n",
    "        self.out = tf.nn.softmax(z3)\n",
    "        self.tf_X = tf_X\n",
    "        self.is_training = is_training\n",
    "\n",
    "    def predict(self, X):\n",
    "        return sess.run(self.out, feed_dict={self.tf_X: X, self.is_training: False})\n",
    "\n",
    "    def activation_func(self, tensor):\n",
    "        if self.activate_f == 'sigmoid':\n",
    "            return tf.sigmoid(tensor)\n",
    "        elif self.activate_f == 'relu':\n",
    "            return tf.nn.relu(tensor)\n",
    "        else:\n",
    "            return tf.sigmoid(tensor)\n",
    "\n",
    "    def _binarize_labels(self, labels):\n",
    "        binarized_labels = np.zeros([labels.shape[0], int(np.max(labels) + 1)])\n",
    "        for i, label in enumerate(labels):\n",
    "            binarized_labels[i, int(label)] = 1.0\n",
    "        return binarized_labels\n",
    "    \n",
    "    def _next_batch(self, index, batch_size, data):\n",
    "        size = len(data)\n",
    "        if index >= size:\n",
    "            raise Exception('index can not be greater than or equal to data size')\n",
    "        if batch_size > size:\n",
    "            raise Exception('batch size can not be greater than data size')\n",
    "\n",
    "        if (index + batch_size) <= size:\n",
    "            new_index = index + batch_size\n",
    "            if new_index == size:\n",
    "                new_index = 0\n",
    "            return new_index, data[index:(index+batch_size),]\n",
    "        else:\n",
    "            tail_data = data[index:size,]\n",
    "            remain_batch_size = batch_size - (size - index)\n",
    "            new_index, remain_data = self._next_batch(0, remain_batch_size, data)\n",
    "            return new_index, np.concatenate((tail_data, remain_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5000: 0.003765s. Loss: 0.443941\n",
      "Epoch 10000: 0.003929s. Loss: 0.564916\n",
      "Epoch 15000: 0.004132s. Loss: 0.439606\n",
      "Epoch 20000: 0.004074s. Loss: 0.397635\n",
      "Epoch 25000: 0.003282s. Loss: 0.484358\n",
      "Epoch 30000: 0.004387s. Loss: 0.351625\n",
      "Epoch 35000: 0.003031s. Loss: 0.455024\n",
      "Epoch 40000: 0.005947s. Loss: 0.463563\n",
      "Epoch 45000: 0.003098s. Loss: 0.414384\n",
      "Epoch 50000: 0.004175s. Loss: 0.464590\n"
     ]
    }
   ],
   "source": [
    "nn = NeuralNet(epoch=50000, batch_size=100, learning_rate=0.5, activate_f='relu')\n",
    "nn.fit(dev_train_features, dev_train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.47780829755934562"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_loss_dup(dev_test_target, nn.predict(dev_test_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
