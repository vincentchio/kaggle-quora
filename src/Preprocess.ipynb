{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import config\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import re\n",
    "import scipy as sp\n",
    "import xgboost as xgb\n",
    "\n",
    "from functools import partial\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.tokenize import word_tokenize, TreebankWordTokenizer\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, \\\n",
    "    ExtraTreesClassifier, VotingClassifier\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss, accuracy_score, confusion_matrix, make_scorer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler, FunctionTransformer\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from utils.feature_utils import load_features, load_target, calibrate, log_loss_dup\n",
    "from utils.file_utils import save_to_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_train_csv_as_rdd(name):\n",
    "    return sc.parallelize([{\n",
    "        'id': r.id, 'qid1': r.qid1, 'qid2': r.qid2, 'question1': r.question1, 'question2': r.question2, 'is_duplicate': r.is_duplicate\n",
    "    } for r in pd.read_csv(name, keep_default_na=False).itertuples()])\n",
    "\n",
    "def read_test_csv_as_rdd(name):\n",
    "    return sc.parallelize([{\n",
    "        'test_id': r.test_id, 'question1': r.question1, 'question2': r.question2\n",
    "    } for r in pd.read_csv(name, keep_default_na=False).itertuples()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('data/train.csv', keep_default_na=False)\n",
    "test_df = pd.read_csv('data/test.csv', keep_default_na=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('data/train.json', 'w') as train_json:\n",
    "    for r in train_df.itertuples():\n",
    "        json.dump(\n",
    "            {\n",
    "                'id': r.id, 'qid1': r.qid1, 'qid2': r.qid2, 'question1': r.question1, 'question2': r.question2, 'is_duplicate': r.is_duplicate\n",
    "            },\n",
    "            train_json)\n",
    "        train_json.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('data/test.json', 'w') as test_json:\n",
    "    for r in test_df.itertuples():\n",
    "        json.dump(\n",
    "            {\n",
    "                'test_id': r.test_id, 'question1': r.question1, 'question2': r.question2\n",
    "            },\n",
    "            test_json)\n",
    "        test_json.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from data_preprocess import preprocess_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = sc.textFile(os.path.join(config.DATA_DIR, 'train.json')).map(lambda r: json.loads(r))\n",
    "test_data = sc.textFile(os.path.join(config.DATA_DIR, 'test.json')).map(lambda r: json.loads(r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_question(r):\n",
    "    r['question1'] = preprocess_text(r['question1'])\n",
    "    r['question2'] = preprocess_text(r['question2'])\n",
    "    return r\n",
    "\n",
    "cleaned_train_data = train_data.map(clean_question)\n",
    "cleaned_test_data = test_data.map(clean_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_to_csv(pd.read_json(json.dumps(cleaned_train_data.collect())), os.path.join(config.DATA_DIR, 'clean_train.csv'))\n",
    "save_to_csv(pd.read_json(json.dumps(cleaned_test_data.collect())), os.path.join(config.DATA_DIR, 'clean_test.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "INCLUDE_TEST = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transform_feature(r, feature_transform):\n",
    "    r[feature_transform.feature_name()] = feature_transform.transform(r['question1'], r['question2'])\n",
    "    return r\n",
    "\n",
    "def transform_all_feature(rdd, feature_transform, partition=100000):\n",
    "    def generate_feature(data):\n",
    "        new_data = []\n",
    "        question1 = []\n",
    "        question2 = []\n",
    "\n",
    "        for i, r in enumerate(data):\n",
    "            new_data.append(r)\n",
    "            question1.append(r['question1'])\n",
    "            question2.append(r['question2'])\n",
    "\n",
    "            if i % partition == 0:\n",
    "                feature = feature_transform.transform_all(question1, question2)\n",
    "                for j, v in enumerate(feature):\n",
    "                    new_data[j][feature_transform.feature_name()] = v\n",
    "                    yield new_data[j]\n",
    "\n",
    "                new_data = []\n",
    "                question1 = []\n",
    "                question2 = []\n",
    "        \n",
    "        if len(new_data) > 0:\n",
    "            feature = feature_transform.transform_all(question1, question2)\n",
    "            for j, v in enumerate(feature):\n",
    "                new_data[j][feature_transform.feature_name()] = v\n",
    "                yield new_data[j]\n",
    "\n",
    "    return rdd.repartition(1).mapPartitions(generate_feature)\n",
    "\n",
    "def corr(rdd, feature_transform):\n",
    "    feature_name = feature_transform.feature_name()\n",
    "    x_y = rdd.map(lambda r: (r[feature_name], r['is_duplicate'])).collect()\n",
    "    r = pearsonr([x for (x, _) in x_y], [y for (_, y) in x_y])[0]\n",
    "    if math.isnan(r):\n",
    "        return 0\n",
    "    else:\n",
    "        return r\n",
    "    \n",
    "def save_feature_to_csv(rdd, feature_transform, data_format='train', partition=100000):\n",
    "    feature_name = feature_transform.feature_name()\n",
    "    def handle_data(data):\n",
    "        with open(os.path.join(config.FEATURE_DIR, data_format, '%s.csv' % feature_name), 'wa') as out:\n",
    "            out_data = []\n",
    "            for i, r in enumerate(data):\n",
    "                new_r = {'id': r['id']} if data_format == 'train' else {'test_id': r['test_id']}\n",
    "                new_r[feature_name] = r[feature_name]\n",
    "                out_data.append(new_r)\n",
    "                if i == 0:\n",
    "                    pd.read_json(json.dumps(out_data)).to_csv(out, index=False, quoting=csv.QUOTE_ALL, header=True)\n",
    "                    out_data = []\n",
    "                elif i % partition == 0:\n",
    "                    print 'Testing: ', i\n",
    "                    pd.read_json(json.dumps(out_data)).to_csv(out, index=False, quoting=csv.QUOTE_ALL, header=False)\n",
    "                    out_data = []\n",
    "            \n",
    "            if len(out_data) > 0:\n",
    "                pd.read_json(json.dumps(out_data)).to_csv(out, index=False, quoting=csv.QUOTE_ALL, header=False)\n",
    "        \n",
    "        yield 'Done'\n",
    "                    \n",
    "\n",
    "    if rdd.getNumPartitions() != 1:\n",
    "        rdd = rdd.repartition(1)\n",
    "\n",
    "    rdd.mapPartitions(handle_data).count()\n",
    "\n",
    "def corpus_rdd(train_data, test_data):\n",
    "    def yield_train_question(r):\n",
    "        yield (r['qid1'], r['question1'])\n",
    "        yield (r['qid2'], r['question2'])\n",
    "    \n",
    "    def yield_test_question(r):\n",
    "        yield r['question1']\n",
    "        yield r['question2']\n",
    "    return train_data.flatMap(yield_train_question).\\\n",
    "        reduceByKey(lambda a, _: a).\\\n",
    "        values().\\\n",
    "        union(test_data.flatMap(yield_test_question)).\\\n",
    "        distinct()\n",
    "\n",
    "def save_corpus(corpus):\n",
    "    with open(os.path.join(config.DATA_DIR, 'corpus.txt'), 'w') as f:\n",
    "        for question in corpus:\n",
    "            f.write(question + '\\n')\n",
    "    \n",
    "def read_corpus():\n",
    "    corpus = []\n",
    "    with open(os.path.join(config.DATA_DIR, 'corpus.txt')) as f:\n",
    "        for line in f:\n",
    "            corpus.append(line[:-1])\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = read_train_csv_as_rdd(os.path.join(config.DATA_DIR, 'clean_train.csv'))\n",
    "test_data = read_test_csv_as_rdd(os.path.join(config.DATA_DIR, 'clean_test.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus_data = corpus_rdd(train_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4780574"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jaccard feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from feature_jaccard import JaccardDistanceTransform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corr for 1-gram: 0.346723\n",
      "Partitions:  8\n",
      "Testing:  1\n",
      "Testing:  1\n",
      "Corr for 2-gram: 0.220864\n",
      "Partitions:  8\n",
      "Testing:  1\n",
      "Testing:  1\n",
      "Corr for 3-gram: 0.142528\n",
      "Partitions:  8\n",
      "Testing:  1\n",
      "Testing:  1\n"
     ]
    }
   ],
   "source": [
    "def generate_jaccard_feature(include_test = False):\n",
    "    ngrams = [1, 2, 3]\n",
    "    for ngram in ngrams:\n",
    "        jaccard_distance_transform = JaccardDistanceTransform(ngram)\n",
    "        train_data_features = train_data.\\\n",
    "            map(lambda r: transform_feature(r, jaccard_distance_transform))\n",
    "        print \"Corr for %d-gram: %f\" % (ngram, corr(train_data_features, jaccard_distance_transform))\n",
    "        save_feature_to_csv(train_data_features, jaccard_distance_transform, 'train')\n",
    "\n",
    "        if include_test:\n",
    "            test_data_features = test_data.\\\n",
    "                map(lambda r: transform_feature(r, jaccard_distance_transform))\n",
    "            save_feature_to_csv(test_data_features, jaccard_distance_transform, 'test')\n",
    "\n",
    "generate_jaccard_feature(INCLUDE_TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Corr for 1-gram: 0.346723\n",
    "# Corr for 2-gram: 0.220864\n",
    "# Corr for 3-gram: 0.142528"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFIDF feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from feature_vector import TfidfCosineSimilarityTransform, TfidfCharCosineSimilarityTransform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corr for 1-gram: 0.407039\n",
      "Corr for 2-gram: 0.209158\n",
      "Corr for 3-gram: 0.149541\n"
     ]
    }
   ],
   "source": [
    "def generate_tfidf_cosince_similarity_feature(include_test=False):\n",
    "    ngrams = [1, 2, 3]\n",
    "    for ngram in ngrams:\n",
    "        tfidf_cosine_similarity_transform = TfidfCosineSimilarityTransform(corpus_data.collect(), ngram)\n",
    "        train_data_features = transform_all_feature(train_data, tfidf_cosine_similarity_transform, partition=50000)\n",
    "        print \"Corr for %d-gram: %f\" % (ngram, corr(train_data_features, tfidf_cosine_similarity_transform))\n",
    "        save_feature_to_csv(train_data_features, tfidf_cosine_similarity_transform, 'train')\n",
    "\n",
    "        if include_test:\n",
    "            test_data_features = transform_all_feature(test_data, tfidf_cosine_similarity_transform, partition=50000)\n",
    "            save_feature_to_csv(test_data_features, tfidf_cosine_similarity_transform, 'test')\n",
    "\n",
    "generate_tfidf_cosince_similarity_feature(INCLUDE_TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Corr for 1-gram: 0.407039\n",
    "# Corr for 2-gram: 0.209158\n",
    "# Corr for 3-gram: 0.149541"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_tfidf_char_cosince_similarity_feature(include_test=False):\n",
    "    ngrams = [3]\n",
    "    for ngram in ngrams:\n",
    "        tfidf_char_cosine_similarity_transform = TfidfCharCosineSimilarityTransform(corpus_data.collect(), ngram)\n",
    "        train_data_features = transform_all_feature(train_data, tfidf_char_cosine_similarity_transform)\n",
    "        print \"Corr for %d-gram: %f\" % (ngram, corr(train_data_features, tfidf_char_cosine_similarity_transform))\n",
    "        save_feature_to_csv(train_data_features, tfidf_char_cosine_similarity_transform, 'train')\n",
    "\n",
    "        if include_test:\n",
    "            test_data_features = transform_all_feature(test_data, tfidf_char_cosine_similarity_transform)\n",
    "            save_feature_to_csv(test_data_features, tfidf_char_cosine_similarity_transform, 'test')\n",
    "\n",
    "generate_tfidf_char_cosince_similarity_feature(INCLUDE_TEST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSA feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from feature_vector import LSACosineSimilarityTransform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corr for 1-gram: 0.301804\n",
      "Corr for 2-gram: 0.182017\n",
      "Corr for 3-gram: 0.159962\n"
     ]
    }
   ],
   "source": [
    "def generate_lsa_cosince_similarity_feature(include_test=False):\n",
    "    ngrams = [1, 2, 3]\n",
    "    for ngram in ngrams:\n",
    "        lsa_cosine_similarity_transform = LSACosineSimilarityTransform(corpus_data.collect(), ngram)\n",
    "        train_data_features = transform_all_feature(train_data, lsa_cosine_similarity_transform)\n",
    "        print \"Corr for %d-gram: %f\" % (ngram, corr(train_data_features, lsa_cosine_similarity_transform))\n",
    "        save_feature_to_csv(train_data_features, lsa_cosine_similarity_transform, 'train')\n",
    "\n",
    "        if include_test:\n",
    "            test_data_features = transform_all_feature(test_data, lsa_cosine_similarity_transform)\n",
    "            save_feature_to_csv(test_data_features, lsa_cosine_similarity_transform, 'test')\n",
    "\n",
    "generate_lsa_cosince_similarity_feature(INCLUDE_TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Corr for 1-gram: 0.301804\n",
    "# Corr for 2-gram: 0.182017\n",
    "# Corr for 3-gram: 0.159962"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Stacking features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from utils.feature_utils import generate_stacking_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_features = load_features()\n",
    "train_target = load_target()\n",
    "dev_train_features, dev_test_features, dev_train_target, dev_test_target = train_test_split(\n",
    "    train_features, train_target, test_size=config.TEST_SIZE, random_state=config.RANDOM_SEED\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Random Forest Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from feature_random_forest_stacking import RandomForestStacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "generate_stacking_feature(RandomForestStacking(), train_features, train_target, load_features('test'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGB Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from feature_xgb_stacking import XGBStacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "utils/feature_utils.py:44: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  train_stacking_features[stacking.feature_name()] = stacking.fit_transform(train_X, train_y)\n",
      "utils/feature_utils.py:52: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  test_stacking_features[stacking.feature_name()] = stacking.transform(test_X)\n"
     ]
    }
   ],
   "source": [
    "generate_stacking_feature(XGBStacking(), train_features, train_target, load_features('test'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from feature_gradient_boosting_stacking import GradientBoostingStacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "generate_stacking_feature(GradientBoostingStacking(), train_features, train_target, load_features('test'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Tree Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from feature_extra_trees_stacking import ExtraTreesStacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "generate_stacking_feature(ExtraTreesStacking(), train_features, train_target, load_features('test'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/features/counts/part-00000-counts.csv\n",
      "data/features/fuzzy_word/part-00000-b6cc8e1c-9d21-4868-adfc-22534a908bb2.csv\n",
      "data/features/interrogative_forms/part-00000-066ad0c6-1dfb-481f-8b1b-57ca6190ba9c.csv\n",
      "data/features/recall_precision_talmi/part-00000-cd79d36b-bd37-4436-9096-39d7c42c03a8.csv\n"
     ]
    }
   ],
   "source": [
    "# def load_other_features():\n",
    "#     features = ['counts', 'fuzzy_word', 'interrogative_forms', 'recall_precision_talmi']\n",
    "    \n",
    "#     feature_df = None\n",
    "#     for feature in features:\n",
    "#         for root, dirs, filenames in os.walk(os.path.join(config.FEATURE_DIR, feature)):\n",
    "#             for filename in filenames:\n",
    "#                 if filename.endswith('.csv'):\n",
    "#                     print os.path.join(root, filename)\n",
    "#                     df = pd.read_csv(os.path.join(root, filename))\n",
    "#                     if feature_df is None:\n",
    "#                         feature_df = df\n",
    "#                     else:\n",
    "#                         feature_df = feature_df.merge(df, on='id')\n",
    "#     return feature_df\n",
    "\n",
    "# train_other_features = load_other_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_count_features():\n",
    "    dfs = []\n",
    "    for root, dirs, filenames in os.walk(os.path.join(config.FEATURE_DIR, 'counts')):\n",
    "        for filename in filenames:\n",
    "            if filename.endswith('.csv'):\n",
    "                dfs.append(pd.read_csv(os.path.join(root, filename)))\n",
    "    return pd.concat(dfs)\n",
    "\n",
    "train_count_features = load_count_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_features = load_features(feature_type='all')\n",
    "train_target = load_target()\n",
    "dev_train_features, dev_test_features, dev_train_target, dev_test_target = train_test_split(\n",
    "    train_features, train_target, test_size=config.TEST_SIZE, random_state=config.RANDOM_SEED\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/ipykernel_launcher.py:2: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=100, n_jobs=1, oob_score=False, random_state=2017,\n",
       "            verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_estimators=100, random_state=config.RANDOM_SEED)\n",
    "rf.fit(dev_train_features.drop('id', 1), dev_train_target.drop('id', 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.45057960035246419"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_loss_dup(dev_test_target.drop('id', 1), rf.predict_proba(dev_test_features.drop('id', 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/sklearn/preprocessing/label.py:112: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/usr/local/lib/python2.7/site-packages/sklearn/preprocessing/label.py:147: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.75,\n",
       "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=9,\n",
       "       min_child_weight=1, missing=None, n_estimators=500, nthread=16,\n",
       "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
       "       scale_pos_weight=1, seed=0, silent=True, subsample=0.75)"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_params = {\n",
    "    'max_depth': 9, \n",
    "    'learning_rate': 0.1,\n",
    "    'n_estimators': 500, \n",
    "    'objective': 'binary:logistic',\n",
    "    'nthread': 16, \n",
    "    'gamma': 0, \n",
    "    'subsample': 0.75, \n",
    "    'colsample_bytree': 0.75, \n",
    "    'colsample_bylevel': 1,\n",
    "    'reg_alpha': 0, \n",
    "    'reg_lambda': 1, \n",
    "    'scale_pos_weight': 1\n",
    "}\n",
    "xgb_cls = xgb.XGBClassifier(**xgb_params)\n",
    "xgb_cls.fit(dev_train_features.drop('id', 1), dev_train_target.drop('id', 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.44462600520222095"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_loss_dup(dev_test_target.drop('id', 1), xgb_cls.predict_proba(dev_test_features.drop('id', 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "              min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "              n_estimators=200, presort='auto', random_state=2017,\n",
       "              subsample=1.0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbc = GradientBoostingClassifier(n_estimators=500, random_state=config.RANDOM_SEED)\n",
    "gbc.fit(dev_train_features.drop('id', 1), dev_train_target.drop('id', 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.47078095238119649"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_loss_dup(dev_test_target.drop('id', 1), gbc.predict_proba(dev_test_features.drop('id', 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
       "          learning_rate=0.0001, n_estimators=300, random_state=2017)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abc = AdaBoostClassifier(n_estimators=300, learning_rate=0.0001, random_state=config.RANDOM_SEED)\n",
    "abc.fit(dev_train_features.drop('id', 1), dev_train_target.drop('id', 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.55022798901810421"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_loss_dup(dev_test_target.drop('id', 1), abc.predict_proba(dev_test_features.drop('id', 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/ipykernel_launcher.py:7: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  import sys\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='entropy',\n",
       "           max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_split=1e-06, min_samples_leaf=2,\n",
       "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "           n_estimators=100, n_jobs=1, oob_score=False, random_state=2017,\n",
       "           verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "etc = ExtraTreesClassifier(\n",
    "    n_estimators=100,\n",
    "    criterion='entropy',\n",
    "    min_samples_leaf=2,\n",
    "    random_state=config.RANDOM_SEED)\n",
    "etc.fit(dev_train_features.drop('id', 1), dev_train_target.drop('id', 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.45021462510499716"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_loss_dup(dev_test_target.drop('id', 1), etc.predict_proba(dev_test_features.drop('id', 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from utils.feature_utils import calibrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_features = load_features(feature_type='all')\n",
    "train_target = load_target()\n",
    "test_features = load_features(data_format='test', feature_type='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('rf', RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_...istic', reg_alpha=0, reg_lambda=1,\n",
       "       scale_pos_weight=1, seed=0, silent=True, subsample=0.75))],\n",
       "         n_jobs=1, voting='soft', weights=[0.5, 0.5])"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_model = RandomForestClassifier(n_estimators=200)\n",
    "final_model = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('rf', RandomForestClassifier(n_estimators=100, random_state=config.RANDOM_SEED)),\n",
    "        ('xgb', xgb.XGBClassifier(**xgb_params))\n",
    "    ],\n",
    "    voting='soft',\n",
    "    weights=[0.5, 0.5])\n",
    "final_model.fit(train_features.drop('id', 1), train_target.drop('id', 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.22524900234832992"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_loss_dup(train_target.drop('id', 1), final_model.predict_proba(train_features.drop('id', 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_submission = test_features[['test_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "test_submission['is_duplicate'] = final_model.predict_proba(test_features.drop('test_id', 1))[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "test_submission['is_duplicate'] = test_submission['is_duplicate'].apply(lambda r: calibrate(r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_to_csv(test_submission, os.path.join(config.DATA_DIR, 'submission', 'submission.csv'), quoting=csv.QUOTE_MINIMAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_to_csv(load_features('train'), os.path.join(config.DATA_DIR, 'train_vincent.csv', 'part-00000-vincent.csv'), quoting=csv.QUOTE_MINIMAL)\n",
    "save_to_csv(load_features('test'), os.path.join(config.DATA_DIR, 'test_vincent.csv', 'part-00000-vincent.csv'), quoting=csv.QUOTE_MINIMAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
